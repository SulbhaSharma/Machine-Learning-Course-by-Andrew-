1. You are training a classification model with logistic regression. Which of the following statements are true? Check all that apply.

     a)Adding a new feature to the model always results in equal or better performance on examples not in the training set.
     b)Introducing regularization to the model always results in equal or better performance on examples not in the training set.
     c)Introducing regularization to the model always results in equal or better performance on the training set.
     d)Adding many new features to the model makes it more likely to overfit the training set.
     Explanation: By introducing too much regularisation to the model, underfitting may take place and will result in worse performance.
                  Also, by adding too many new features it results in overfitting.
     
2. Suppose you ran logistic regression twice, once with λ=0, and once with λ=1. One of the times, you got parameters θ=74.8145.05, and the other time you got θ=1.370.51. However, you forgot which value of
    λ corresponds to which value of θ. Which one do you think corresponds to λ=1?
       (i) θ=1.370.51
       (ii)θ=74.8145.05
       
3. Which of the following statements about regularization are true? Check all that apply.

     a)Using too large a value of λ can cause your hypothesis to overfit the data; this can be avoided by reducing λ.
     b)Because logistic regression outputs values 0≤hθ(x)≤1, its range of output values can only be "shrunk" slightly by regularization anyway, so regularization is generally not helpful for it.
     c)Consider a classification problem. Adding regularization may cause your classifier to incorrectly classify some training examples (which it had correctly classified when not using regularization, i.e. when λ=0).
     d)Using a very large value of λ cannot hurt the performance of your hypothesis; the only reason we do not set λ to be too large is to avoid numerical problems.
     
     Explanation:Using too large value of lambda will result in underfitting.
                 Regularisation affects parameter theta and is beneficial for logistic regression.
                 Since, regularisation penalizes complex model that is which has large theta values then it would reduce the number of features
                 and result in wrong classification

4. In which one of the following figures do you think the hypothesis has overfit the training set?
   The figure in which line passes through every given points.


5. In which one of the following figures do you think the hypothesis has underfit the training set?
   The figure in which line passes through least number of points.
