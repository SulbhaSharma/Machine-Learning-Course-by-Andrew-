+1. Suppose that you have trained a logistic regression classifier, and it outputs on a new example x a prediction hθ(x) = 0.7. 
+This means (check all that apply):
+
+
+    a)Our estimate forP(y=1|x;θ) is 0.7.
+    b)Our estimate forP(y=0|x;θ) is 0.3.
+    c)Our estimate forP(y=1|x;θ) is 0.3.
+    d)Our estimate forP(y=0|x;θ) is 0.7.
+    
+    Explanation: Since, for P(y=1|x;θ) is 0.7.  
+                 hθ(x)is preciselyP(y=1|x;θ) , so each is 0.7. 
+                 As, P(y=0|x;θ) = 1−P(y=1|x;θ) , then it will be 1−0.7=0.3 
+
+2. Suppose you have the following training set, and fit a logistic regression classifier hθ(x)=g(θ0+θ1x1+θ2x2).
+Which of the following are true? Check all that apply.
+
+
+    a)Adding polynomial features (e.g., instead using h(x)=g(0+1x1+2x2+3x12+4x1x2+5x22) ) could increase how well we can fit the
+      training data.
+    b)The positive and negative examples cannot be separated using a straight line. So, gradient descent will fail to converge.
+    c)At the optimal value ofθ (e.g., found by fminunc), we will haveJ(θ)≥0 .
+    d)Because the positive and negative examples cannot be separated using a straight line, linear regression will perform as well 
+      as logistic regression on this data.
+
+   Explanation: By, adding new features it can improve the fitting on its training set as by applying θ3=θ4=θ5=0 makes the hypothesis 
+                the same as the originally it was, then the algorithm of gradient descent will use those features if it would allow the 
+                training set to fit more perfectly.
+                Also, for logistic regression the cost functionJ(θ) is always positive.
+                .
+3. For logistic regression, the gradient is given by ∂∂θjJ(θ)=1m∑i=1m(hθ(x(i))−y(i))xj(i). 
+   Which of these is a correct gradient descent update for logistic regression with a learning rate of α? Check all that apply.
+
+   Explanation: It is shown θj:=θj−α/m (m∑i=1)(hθ(x(i))−y(i))x(i)j 
+                Replacing hθ(x(i) by 1/1+e^−θTx(i), we get
+                  θj:=θj−α1m∑mi=1(11+e−θTx(i)−y(i))x(i)j 
+4. Which of the following statements are true? Check all that apply.
+    a)For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum).
+      This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc). 
+      0.00 The cost function for logistic regression is convex, so gradient descent will always converge to the global minimum. 
+      We still might use a more advanded optimization algorithm since they can be faster and don't require you to select a learning
+      rate.
+    b)Since we train one classifier when there are two classes, we train two classifiers when there are three classes (and we do
+      one-vs-all classification).
+    c)The cost function J(θ) for logistic regression trained with m >=1 examples is always greater than or equal to zero.
+    d)The one-vs-all technique allows you to use logistic regression for problems in which each y(i) comes from a fixed, 
+      discrete set of values.
+    
+    Explanation:For multi-class classification or one-vs-all classification problem with k classes y belong to {1,2,3,...k}. Using, logistic
+                regression, k different logistic regression classifiers will be trained.
+                Also, the one-vs-all technique allows you to use logistic regression for problems in which each y(i) comes from 
+                a fixed,discrete set of values. This happens as for each y(i) from one of the k different values, we can label it 
+                as {1,2,…,k}.
+                Lastly, the cost function J(θ) is always positive.
+5. Suppose you train a logistic classifier hθ(x)=g(θ0+θ1x1+θ2x2). Suppose θ0=6,θ1=−1,θ2=0. Which of the following figures 
+   represents the decision boundary found by your classifier?
+    
+    a)1 | 0 vertical
+    b) 0 | 1 vertical
+    c) 0 | 1 horizontal
+    d) 1 | 0 horizontal
+   Explanation:Predict 1)'y=1' if hθ(x)>=0.5.
+                       2)'y=0' if hθ(x)<0.5.
+                               
+                  Case 1: For predicting 'y=1'
+                            Consider, hθ(x)>=0.5 
+                               when x>=0.
+                  Case 2: For predicting 'y=2'
+                            Consider, hθ(x)<0.5 
+                               when x<0   
